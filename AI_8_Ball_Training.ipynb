{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI 8-Ball Training",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/ai8ball/blob/main/AI_8_Ball_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCi7CzAF8D9t"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install wikipedia\n",
        "!pip install pynytimes\n",
        "!gsutil cp gs://boolq/train.jsonl .\n",
        "!gsutil cp gs://boolq/dev.jsonl ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1RTMjJu8J9W"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bA3NVJK8Nnv"
      },
      "source": [
        "# Use a GPU if you have one available (Runtime -> Change runtime type -> GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(26)\n",
        "np.random.seed(26)\n",
        "torch.manual_seed(26)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\") \n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\")\n",
        "model.to(device) # Send the model to the GPU if we have one\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2nMOH1T8Ry2"
      },
      "source": [
        "def encode_data(tokenizer, questions, passages, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for question, passage in zip(questions, passages):\n",
        "        encoded_data = tokenizer.encode_plus(question, passage,\n",
        "          max_length=max_length, pad_to_max_length=True,\n",
        "          truncation_strategy=\"longest_first\")\n",
        "        encoded_pair = encoded_data[\"input_ids\"]\n",
        "        attention_mask = encoded_data[\"attention_mask\"]\n",
        "\n",
        "        input_ids.append(encoded_pair)\n",
        "        attention_masks.append(attention_mask)\n",
        "    return np.array(input_ids), np.array(attention_masks)\n",
        "\n",
        "# Loading data\n",
        "train_data_df = pd.read_json(\"/content/train.jsonl\", lines=True, orient='records')\n",
        "dev_data_df = pd.read_json(\"/content/dev.jsonl\", lines=True, orient=\"records\")\n",
        "passages_train = train_data_df.passage.values\n",
        "questions_train = train_data_df.question.values\n",
        "answers_train = train_data_df.answer.values.astype(int)\n",
        "passages_dev = dev_data_df.passage.values\n",
        "questions_dev = dev_data_df.question.values\n",
        "answers_dev = dev_data_df.answer.values.astype(int)\n",
        "\n",
        "# Encoding data\n",
        "max_seq_length = 256\n",
        "input_ids_train, attention_masks_train = encode_data(tokenizer, questions_train, passages_train, max_seq_length)\n",
        "input_ids_dev, attention_masks_dev = encode_data(tokenizer, questions_dev, passages_dev, max_seq_length)\n",
        "train_features = (input_ids_train, attention_masks_train, answers_train)\n",
        "dev_features = (input_ids_dev, attention_masks_dev, answers_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTUx_TOf8WFt"
      },
      "source": [
        "batch_size = 8\n",
        "train_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in train_features]\n",
        "dev_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in dev_features]\n",
        "train_dataset = TensorDataset(*train_features_tensors)\n",
        "dev_dataset = TensorDataset(*dev_features_tensors)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "dev_sampler = SequentialSampler(dev_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdVSNkU68ZJV"
      },
      "source": [
        "from tqdm import tqdm\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "grad_acc_steps = 4\n",
        "train_loss_values = []\n",
        "dev_acc_values = []\n",
        "for _ in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "  # Training\n",
        "  epoch_train_loss = 0\n",
        "  model.train()\n",
        "  model.zero_grad()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "      input_ids = batch[0].to(device)\n",
        "      attention_masks = batch[1].to(device)\n",
        "      labels = batch[2].to(device)     \n",
        "      outputs = model(input_ids, token_type_ids=None,\n",
        "                      attention_mask=attention_masks, labels=labels)\n",
        "      loss = outputs[0]\n",
        "      loss = loss / grad_acc_steps\n",
        "      epoch_train_loss += loss.item()\n",
        "      loss.backward()\n",
        "      if (step+1) % grad_acc_steps == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "  epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
        "  train_loss_values.append(epoch_train_loss)\n",
        "  \n",
        "  # Evaluation\n",
        "  epoch_dev_accuracy = 0\n",
        "  model.eval()\n",
        "  for batch in dev_dataloader:\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2]   \n",
        "    with torch.no_grad():        \n",
        "        outputs = model(input_ids, token_type_ids=None,\n",
        "                        attention_mask=attention_masks)          \n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions = np.argmax(logits, axis=1).flatten()\n",
        "    labels = labels.numpy().flatten()\n",
        "    epoch_dev_accuracy += np.sum(predictions == labels) / len(labels)\n",
        "\n",
        "  epoch_dev_accuracy = epoch_dev_accuracy / len(dev_dataloader)\n",
        "  dev_acc_values.append(epoch_dev_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffTQcv72s-uE"
      },
      "source": [
        "!mkdir roberta-large_fine-tuned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IMMYhpqb_7D"
      },
      "source": [
        "import os\n",
        "model_path = \"roberta-large_fine-tuned\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um6Yo4fL4shJ"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set()\n",
        "plt.plot(train_loss_values, label=\"train_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend()\n",
        "plt.xticks(np.arange(0, epochs))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56tvW7rs5REf"
      },
      "source": [
        "plt.plot(dev_acc_values, label=\"dev_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Evaluation Accuracy\")\n",
        "plt.legend()\n",
        "plt.xticks(np.arange(0, epochs))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIq27C0o8qs2"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "def predict(question, passage):\n",
        "  sequence = tokenizer.encode_plus(question, passage, return_tensors=\"pt\",\n",
        "    max_length=512, truncation=True)['input_ids'].to(device)\n",
        "  logits = model(sequence)[0]\n",
        "  probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n",
        "  vector = logits.detach().cpu().tolist()[0]\n",
        "  confidence = min(math.sqrt(vector[0]**2+vector[1]**2)/3.6, 1)\n",
        "  proba_yes = round(probabilities[1], 2)\n",
        "  proba_no = round(probabilities[0], 2)\n",
        "  conf_round = round(confidence, 2)\n",
        "  print(\"Question:\", question, \"Yes:\", proba_yes, \"No:\", proba_no, \"Conf.:\",\n",
        "    conf_round)\n",
        "  \n",
        "passage_magic_8_ball = \"\"\"The Magic 8-Ball is a plastic sphere, made to look\n",
        "  like an eight-ball, that is used for fortune-telling or seeking advice. It was\n",
        "  invented in 1950 by Albert C. Carter and Abe Bookman and is currently\n",
        "  manufactured by Mattel. The user asks a yes–no question to the ball and then\n",
        "  turns it over to reveal an answer in a window on the ball.\"\"\"\n",
        "\n",
        "magic_8_ball_questions = [\n",
        "  \"Is the Magic 8-Ball a sphere?\", \n",
        "  \"Is the Magic 8-Ball a cube?\",\n",
        "  \"Was the Magic 8-Ball invented in 1940?\", \n",
        "  \"Was the Magic 8-Ball invented in 1950?\", \n",
        "  \"Was the Magic 8-Ball invented by Carter and Bookman?\", \n",
        "  \"Was the Magic 8-Ball invented by Black and Decker?\", \n",
        "]\n",
        "\n",
        "for s_question in magic_8_ball_questions:\n",
        "  predict(s_question, passage_magic_8_ball)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}