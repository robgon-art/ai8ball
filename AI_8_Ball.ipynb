{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI 8-Ball",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/ai8ball/blob/main/AI_8_Ball.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EvOHSsDXUzZ"
      },
      "source": [
        "# **AI 8-Ball**\n",
        "![AI 8-Ball](https://raw.githubusercontent.com/robgon-art/ai8ball/main/ai8ball_med.jpg)</br>\n",
        "The AI 8-Ball can answer yes/no questions by using the power of the Internet and Machine Learning.</br>\n",
        "\n",
        "Please initialize the system by hitting the first Run cell button below. It takes 3 to 5 minutes to set up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCi7CzAF8D9t",
        "cellView": "form"
      },
      "source": [
        "#@title Initialize the System\n",
        "!pip install transformers wikipedia pynytimes jsonlines pytextrank spacy==2.2\n",
        "\n",
        "!gsutil cp gs://boolq/train.jsonl .\n",
        "!gsutil cp gs://boolq/dev.jsonl .\n",
        "!gsutil cp gs://boolq/test.jsonl .\n",
        "!wget -O answers.pkl https://github.com/robgon-art/ai8ball/raw/main/answers.pkl\n",
        "!wget -O roberta-large_fine-tuned.zip --no-check-certificate \"https://onedrive.live.com/download?cid=61FC7243E093B36A&resid=61FC7243E093B36A%211286&authkey=ALT4NEljIrPRmHk\"\n",
        "!wget -O encoded_qs.pkl --no-check-certificate \"https://onedrive.live.com/download?cid=61FC7243E093B36A&resid=61FC7243E093B36A%211287&authkey=ACbwVMsjvMecSls\"\n",
        "!unzip roberta-large_fine-tuned.zip -d roberta-large_fine-tuned\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from codecs import decode, encode\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "import jsonlines\n",
        "from scipy import spatial\n",
        "import spacy.cli\n",
        "import pytextrank\n",
        "import spacy\n",
        "import pickle\n",
        "\n",
        "encoding_module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
        "encoding_model = hub.load(encoding_module_url)\n",
        "def d(t): return decode(t,\"base-64\").decode()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "model_path = \"roberta-large_fine-tuned\"\n",
        "print(\"Loading tokenizer from \" + model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "print(\"Loading model from \" + model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "_ = model.to(device)\n",
        "\n",
        "def predict(question, passage):\n",
        "  if len(question) == 0 or len(passage) == 0:\n",
        "    return 0.5, 0.0\n",
        "  sequence = tokenizer.encode_plus(question, passage, return_tensors=\"pt\",\n",
        "    max_length=512, truncation=True)['input_ids'].to(device)\n",
        "  logits = model(sequence)[0]\n",
        "  probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n",
        "  vector = logits.detach().cpu().tolist()[0]\n",
        "  confidence = min(math.sqrt(vector[0]**2+vector[1]**2)/3.6, 1)\n",
        "  proba_yes = probabilities[1]\n",
        "  confidence = round(confidence, 3)\n",
        "  return proba_yes, confidence\n",
        "\n",
        "def predict_and_print(question, passage):\n",
        "  proba_yes, confidence = predict(question, passage)\n",
        "  print(f\"Question: {question}, Yes: {round(proba_yes,3)}, No: {round(1-proba_yes, 3)}, Confidence {confidence}\")\n",
        "  return proba_yes, confidence\n",
        "\n",
        "spacy_model = \"en_core_web_md\"\n",
        "print(\"Downloading \" + spacy_model)\n",
        "spacy.cli.download(spacy_model)\n",
        "\n",
        "boolq_data = []\n",
        "for file_name in [\"dev.jsonl\", \"test.jsonl\", \"train.jsonl\"]:\n",
        "  with jsonlines.open(file_name) as file:\n",
        "      for line in file.iter():\n",
        "        boolq_data.append(line)\n",
        "def embed(input):\n",
        "  e = encoding_model([input])[0]\n",
        "  proto_tensor = tf.make_tensor_proto(e)\n",
        "  a = tf.make_ndarray(proto_tensor)\n",
        "  return a.tolist()\n",
        "encoded_qs = pickle.load(open(\"encoded_qs.pkl\", \"rb\"))\n",
        "question_tree = spatial.KDTree(encoded_qs)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tr = pytextrank.TextRank()\n",
        "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
        "answers = pickle.load(open(\"answers.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9gv66C900FP"
      },
      "source": [
        "Next, choose one of the sample questions by clicking on the down-arrow, or ask one of your own by typing it in. And then hit the second Run cell button to see the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMUMZFIc1ewU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "70095416-d5dc-4a50-925d-45d1cd5f4d17"
      },
      "source": [
        "question = \"Can a computer beat a grandmaster chess player?\" #@param [\"Can a computer beat a grandmaster chess player?\", \"Can fortunetellers actually predict the future?\", \"Is global warming caused by humans?\", \"Is Schitt's Creek a good show?\", \"Is it true that 'kangaroo' means 'I don't know'?\"] {allow-input: true}\n",
        "\n",
        "query = \"\"\n",
        "keywords = nlp(question.lower())\n",
        "for p in keywords._.phrases[:5]:\n",
        "  if p.rank > 0.01:\n",
        "    query += p.text + \" \"\n",
        "query = query.strip()\n",
        "if len(query) == 0:\n",
        "  query = question\n",
        "\n",
        "import wikipedia\n",
        "print(\"Checking the Wikipedia.\")\n",
        "results = wikipedia.search(query, results = 3)\n",
        "wiki_passage = \"\"\n",
        "for r in results[1:]:\n",
        "  try:\n",
        "    s = wikipedia.summary(r)\n",
        "  except:\n",
        "    continue\n",
        "  wiki_passage += s.strip() + \" \"\n",
        "wiki_passage = wiki_passage.replace(\"\\n\", \" \")\n",
        "wiki_yes, wiki_conf = predict(question, wiki_passage)\n",
        "\n",
        "from pynytimes import NYTAPI\n",
        "nyt_passage = \"\"\n",
        "print(\"Checking the New York Times.\")\n",
        "nyt = NYTAPI(d(b'R2xsdTF4S2lLMjdSc3dBOXZ0VkZwSjMxbmoyS1RjVzM=\\n'))\n",
        "articles = nyt.article_search(query = query, results = 3,\n",
        "  options = {\"sort\": \"relevance\"})\n",
        "for a in articles[:3]:\n",
        "  nyt_passage += a[\"abstract\"].strip() + \" \"\n",
        "  nyt_passage += a[\"lead_paragraph\"].strip() + \" \"\n",
        "nyt_yes, nyt_conf = predict(question, nyt_passage)\n",
        "\n",
        "from scipy import spatial\n",
        "boolq_passage = \"\"\n",
        "print(\"Checking the BoolQ Dataset.\\n\")\n",
        "question_embed = embed(question)\n",
        "result = question_tree.query(question_embed)\n",
        "if (result[0] < 1):\n",
        "  index = result[1]\n",
        "  boolq_passage = boolq_data[result[1]][\"passage\"]\n",
        "  similar_question = boolq_data[result[1]][\"question\"]\n",
        "boolq_yes, boolq_conf = predict(question, boolq_passage)\n",
        "\n",
        "conf = 0\n",
        "yes = 0.5\n",
        "passage = \"\"\n",
        "source = \"no source\"\n",
        "\n",
        "if (wiki_conf > nyt_conf and wiki_conf > boolq_conf):\n",
        "  yes = wiki_yes\n",
        "  conf = wiki_conf\n",
        "  passage = wiki_passage\n",
        "  source = \"Wikipedia\"\n",
        "else:\n",
        "  if (nyt_conf > boolq_conf):\n",
        "    yes = nyt_yes\n",
        "    conf = nyt_conf\n",
        "    passage = nyt_passage\n",
        "    source = \"New York Times\"\n",
        "  else:\n",
        "    yes = boolq_yes\n",
        "    conf = boolq_conf\n",
        "    passage = boolq_passage\n",
        "    source = \"BoolQ Dataset\"\n",
        "\n",
        "import textwrap\n",
        "\n",
        "min_dist = float(\"inf\")\n",
        "pick = 0\n",
        "\n",
        "if (conf < 0.5):\n",
        "  map_conf =  1 +  conf * 19 / 2\n",
        "  for i, a in enumerate(answers[:5]):\n",
        "    c = a[1][1]\n",
        "    distance = abs(map_conf-c)\n",
        "    if distance < min_dist:\n",
        "      min_dist = distance\n",
        "      pick = i\n",
        "else:\n",
        "  map_yes = 1 + (yes * 1.5 if yes > 2/3 else yes) * 19 / 1.5\n",
        "  for i, a in enumerate(answers[5:]):\n",
        "    y = a[1][0]\n",
        "    distance = abs(map_yes-y)\n",
        "    if distance < min_dist:\n",
        "      min_dist = distance\n",
        "      pick = i+5\n",
        "\n",
        "print(\"The AI 8-Ball's Answer:\", answers[pick][0], \"\\n\")\n",
        "print(\"Yes:\", str(round(yes*100, 2)) + \"%\")\n",
        "print(\"No:\", str(round((1-yes)*100, 2)) + \"%\")\n",
        "print(\"Confidence:\", str(round(conf*100, 2)) + \"%\")\n",
        "print(\"Source:\", source)\n",
        "\n",
        "print(textwrap.fill(\"Passage: \" + passage, width=150))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the Wikipedia.\n",
            "Checking the New York Times.\n",
            "Checking the BoolQ Dataset.\n",
            "\n",
            "The AI 8-Ball's Answer: It is certain. \n",
            "\n",
            "Yes: 99.52%\n",
            "No: 0.48%\n",
            "Confidence: 100%\n",
            "Source: BoolQ Dataset\n",
            "Passage: Chess programs running on commercially-available desktop computers had convincing victories against human players in matches in 2005 and\n",
            "2006. Since that time, chess programs running on commercial hardware - more recently including mobile phones - have been able to defeat even the\n",
            "strongest human players.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7L1SkKLIVIt"
      },
      "source": [
        "[![Wikipedia logo](https://raw.githubusercontent.com/robgon-art/ai8ball/main/wikipedia_ai_logo.png)](https://en.wikipedia.org)\n",
        "[![Data Provided by New York Times logo](https://raw.githubusercontent.com/robgon-art/ai8ball/main/poweredby_nytimes_200a.png)]( https://developer.nytimes.com)\n",
        "[![Google AI logo](https://raw.githubusercontent.com/robgon-art/ai8ball/main/google_ai_logo.png)](https://ai.google)"
      ]
    }
  ]
}